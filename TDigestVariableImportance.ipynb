{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\n",
       "//import $profile.`hadoop-2.6`\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     // adjust spark version - spark >= 2.0\n",
       "//import $ivy.`org.apache.hadoop:hadoop-aws:2.6.4`\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mResolvers._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "//import $profile.`hadoop-2.6`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.1.0` // adjust spark version - spark >= 2.0\n",
    "//import $ivy.`org.apache.hadoop:hadoop-aws:2.6.4`\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._\n",
    "import Resolvers._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mvegas._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mvegas.data.External._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.vegas-viz::vegas:0.3.9`\n",
    "import vegas._\n",
    "import vegas.data.External._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interp.resolvers() = interp.resolvers() :+ Resolver.Http(\n",
    "  \"isarn project\",\n",
    "  \"https://dl.bintray.com/isarn/maven/\",\n",
    "  MavenPattern,\n",
    "  true\n",
    ")\n",
    "interp.resolvers() = interp.resolvers() :+ Resolver.Http(\n",
    "  \"Will Benton\",\n",
    "  \"https://dl.bintray.com/willb/maven/\",\n",
    "  MavenPattern,\n",
    "  true\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.isarnproject.sketches._\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.isarnproject::isarn-sketches:0.0.3.rc1`\n",
    "import org.isarnproject.sketches._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                           \u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`com.redhat.et::silex:0.1.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd.RDD\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.tree.RandomForest\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.regression.LabeledPoint\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.classification.ClassificationModel\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.linalg.DenseVector\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.redhat.et.silex.util.vectors.implicits._\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.tree.RandomForest\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.classification.ClassificationModel\n",
    "import org.apache.spark.mllib.linalg.DenseVector\n",
    "\n",
    "import com.redhat.et.silex.util.vectors.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@6ca1dbcf\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msc\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = JupyterSparkSession.builder() // important - call this rather than SparkSession.builder()\n",
    "  .jupyter() // this method must be called straightaway after builder()\n",
    "  .master(\"spark://notebook:7077\")\n",
    "  .appName(\"notebook\")\n",
    "  .getOrCreate()\n",
    "def sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mloadCSV\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loadCSV(fname: String) = spark.read\n",
    "  .format(\"com.databricks.spark.csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"mode\", \"DROPMALFORMED\")\n",
    "  .load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mAddSampling\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit class AddSampling(td: TDigest) extends Serializable {\n",
    "    import org.isarnproject.collections.mixmaps.nearest.Cover\n",
    "    def cdfDiscrete(x: Double): Double = {\n",
    "        td.clusters.coverR(x).l.fold(0.0){ case(x,_) => td.clusters.prefixSum(x) / td.clusters.sum }\n",
    "    }\n",
    "    def cdfDiscreteInverse(q: Double): Double = {\n",
    "        td.clusters.mCover(q * td.clusters.sum).map(_.data.key) match {\n",
    "            case Cover(_, Some(x)) => x\n",
    "            case Cover(Some(x), None) => x\n",
    "        }\n",
    "    }\n",
    "    def sample: Double = {\n",
    "        val clust = td.clusters\n",
    "        td.nclusters match {\n",
    "            case 0 => 0.0\n",
    "            case n if (n <= td.maxDiscrete) => cdfDiscreteInverse(scala.util.Random.nextDouble)\n",
    "            case _ => td.cdfInverse(scala.util.Random.nextDouble)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtdSketchFV\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tdSketchFV(fv: RDD[Array[Double]]): Vector[TDigest] = {\n",
    "    val tds = fv.aggregate(Array.empty[TDigest])(\n",
    "        (tdv, xv) => {\n",
    "            if (tdv.isEmpty) {\n",
    "                Array.tabulate(xv.length) { j => TDigest.empty(maxDiscrete=50) + xv(j) }\n",
    "            } else {\n",
    "                for { j <- 0 until xv.length } { tdv(j) += xv(j) }\n",
    "                tdv\n",
    "            }\n",
    "        },\n",
    "        (tdv1, tdv2) => {\n",
    "            if (tdv1.isEmpty) tdv2 else {\n",
    "                for { j <- 0 until tdv1.length } { tdv1(j) ++= tdv2(j) }\n",
    "                tdv1\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    tds.toVector\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainCSV\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_c0: string, AW: string ... 800 more fields]\n",
       "\u001b[36mtestCSV\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_c0: string, AW: string ... 800 more fields]\n",
       "\u001b[36mtrainLabelsCSV\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_c0: string, NR.AhR: string ... 11 more fields]\n",
       "\u001b[36mtestLabelsCSV\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_c0: string, NR.AhR: string ... 11 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainCSV = loadCSV(\"/data/tox21/tox21_dense_train.csv\")\n",
    "val testCSV = loadCSV(\"/data/tox21/tox21_dense_test.csv\")\n",
    "val trainLabelsCSV = loadCSV(\"/data/tox21/tox21_labels_train.csv\")\n",
    "val testLabelsCSV = loadCSV(\"/data/tox21/tox21_labels_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcsvToPairs\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcsvToLP\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def csvToPairs(csv: DataFrame) =\n",
    "    csv.rdd.map(_.toSeq)\n",
    "    .map(v => (v.head.asInstanceOf[String], v.tail.toVector.map(_.asInstanceOf[String].toDouble)))\n",
    "def csvToLP(tlab: String, labCSV: DataFrame, fvCSV: DataFrame) = {\n",
    "    val labCol = labCSV.select(\"_c0\", tlab)\n",
    "    val labPairs = labCol.filter(labCol(tlab) =!= \"NA\")\n",
    "        .rdd.map(_.toSeq).map { s => (s(0).asInstanceOf[String], s(1).asInstanceOf[String].toDouble) }\n",
    "    val fvPairs = csvToPairs(fvCSV)\n",
    "    val lp = labPairs.join(fvPairs).map { case (_, (lab, feats)) => LabeledPoint(lab, feats.toSpark) }\n",
    "    lp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres12\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"_c0 NR.AhR NR.AR NR.AR.LBD NR.Aromatase NR.ER NR.ER.LBD NR.PPAR.gamma SR.ARE SR.ATAD5 SR.HSE SR.MMP SR.p53\"\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLabelsCSV.columns.mkString(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainLP\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mLabeledPoint\u001b[39m] = MapPartitionsRDD[43] at map at cmd11.sc:9\n",
       "\u001b[36mtestLP\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mLabeledPoint\u001b[39m] = MapPartitionsRDD[59] at map at cmd11.sc:9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainLP = csvToLP(\"`NR.AhR`\", trainLabelsCSV, trainCSV)\n",
    "val testLP = csvToLP(\"`NR.AhR`\", testLabelsCSV, testCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mlrModel\u001b[39m: \u001b[32mLogisticRegressionModel\u001b[39m = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 801, numClasses = 2, threshold = 0.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrModel = new LogisticRegressionWithLBFGS().run(trainLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mvariableImportance\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def variableImportance(\n",
    "    model: ClassificationModel,\n",
    "    lp: RDD[LabeledPoint],\n",
    "    sc: SparkContext = spark.sparkContext): Array[Double] = {\n",
    "\n",
    "    // sketch each feature using t-digest\n",
    "    val fvtd = tdSketchFV(lp.map(_.features.toArray))\n",
    "    val m = fvtd.length\n",
    "    val n = fvtd.head.clusters.sum\n",
    "    // broadcast the sketches\n",
    "    val tdBC = sc.broadcast(fvtd)\n",
    "    // count how many predictions differ, by feature,\n",
    "    // when a randomized feature value is used\n",
    "    val dcounts = lp.aggregate(Array.fill(m)(0L))(\n",
    "        (counts, lpt) => {\n",
    "            val LabeledPoint(lab, feats) = lpt\n",
    "            val refpred = model.predict(feats)\n",
    "            val fa = feats.toArray\n",
    "            val fv = new DenseVector(fa)\n",
    "            for { j <- 0 until m } {\n",
    "                val t = fa(j)\n",
    "                fa(j) = tdBC.value(j).sample\n",
    "                val pred = model.predict(fv)\n",
    "                fa(j) = t\n",
    "                if (pred != refpred) counts(j) += 1\n",
    "            }\n",
    "            counts\n",
    "        },\n",
    "        (counts1, counts2) => {\n",
    "            for { j <- 0 until m } {\n",
    "                counts1(j) += counts2(j)\n",
    "            }\n",
    "            counts1\n",
    "        }\n",
    "    )\n",
    "    // report differences as a fraction of total samples\n",
    "    dcounts.map(c => c.toDouble / n)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrawVI\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m0.005449591280653951\u001b[39m,\n",
       "  \u001b[32m0.005804999407653121\u001b[39m,\n",
       "  \u001b[32m0.0037910200213244877\u001b[39m,\n",
       "  \u001b[32m0.019784385736287168\u001b[39m,\n",
       "  \u001b[32m0.0022509181376614146\u001b[39m,\n",
       "  \u001b[32m0.002013979386328634\u001b[39m,\n",
       "  \u001b[32m0.007108162539983414\u001b[39m,\n",
       "  \u001b[32m0.00473877502665561\u001b[39m,\n",
       "  \u001b[32m0.00473877502665561\u001b[39m,\n",
       "  \u001b[32m0.006989693164317024\u001b[39m,\n",
       "  \u001b[32m0.0015401018836630732\u001b[39m,\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawVI = variableImportance(lrModel, trainLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mviPairs\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mDouble\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m\"MRVSA9\"\u001b[39m, \u001b[32m0.0363700983295818\u001b[39m),\n",
       "  (\u001b[32m\"slogPVSA9\"\u001b[39m, \u001b[32m0.03281601705959009\u001b[39m),\n",
       "  (\u001b[32m\"slogPVSA10\"\u001b[39m, \u001b[32m0.03257907830825731\u001b[39m),\n",
       "  (\u001b[32m\"RASA\"\u001b[39m, \u001b[32m0.028314180784267267\u001b[39m),\n",
       "  (\u001b[32m\"UI\"\u001b[39m, \u001b[32m0.025589385143940293\u001b[39m),\n",
       "  (\u001b[32m\"RPSA\"\u001b[39m, \u001b[32m0.02476009951427556\u001b[39m),\n",
       "  (\u001b[32m\"PEOEVSA7\"\u001b[39m, \u001b[32m0.024167752635943607\u001b[39m),\n",
       "  (\u001b[32m\"MRVSA2\"\u001b[39m, \u001b[32m0.023575405757611658\u001b[39m),\n",
       "  (\u001b[32m\"FNSA3\"\u001b[39m, \u001b[32m0.022627650752280536\u001b[39m),\n",
       "  (\u001b[32m\"slogPVSA11\"\u001b[39m, \u001b[32m0.022509181376614146\u001b[39m),\n",
       "  (\u001b[32m\"PNSA3\"\u001b[39m, \u001b[32m0.022390712000947756\u001b[39m),\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val viPairs = trainCSV.columns.tail.zip(rawVI).sortBy { case (_, vi) => -vi}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres18\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mDouble\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m\"MRVSA9\"\u001b[39m, \u001b[32m0.0363700983295818\u001b[39m),\n",
       "  (\u001b[32m\"slogPVSA9\"\u001b[39m, \u001b[32m0.03281601705959009\u001b[39m),\n",
       "  (\u001b[32m\"slogPVSA10\"\u001b[39m, \u001b[32m0.03257907830825731\u001b[39m),\n",
       "  (\u001b[32m\"RASA\"\u001b[39m, \u001b[32m0.028314180784267267\u001b[39m),\n",
       "  (\u001b[32m\"UI\"\u001b[39m, \u001b[32m0.025589385143940293\u001b[39m),\n",
       "  (\u001b[32m\"RPSA\"\u001b[39m, \u001b[32m0.02476009951427556\u001b[39m),\n",
       "  (\u001b[32m\"PEOEVSA7\"\u001b[39m, \u001b[32m0.024167752635943607\u001b[39m),\n",
       "  (\u001b[32m\"MRVSA2\"\u001b[39m, \u001b[32m0.023575405757611658\u001b[39m),\n",
       "  (\u001b[32m\"FNSA3\"\u001b[39m, \u001b[32m0.022627650752280536\u001b[39m),\n",
       "  (\u001b[32m\"slogPVSA11\"\u001b[39m, \u001b[32m0.022509181376614146\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viPairs.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres19\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mDouble\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m\"RDFE14\"\u001b[39m, \u001b[32m0.0\u001b[39m),\n",
       "  (\u001b[32m\"RDFE20\"\u001b[39m, \u001b[32m0.0\u001b[39m),\n",
       "  (\u001b[32m\"RDFE22\"\u001b[39m, \u001b[32m0.0\u001b[39m),\n",
       "  (\u001b[32m\"RDFE29\"\u001b[39m, \u001b[32m0.0\u001b[39m),\n",
       "  (\u001b[32m\"RDFP25\"\u001b[39m, \u001b[32m0.0\u001b[39m),\n",
       "  (\u001b[32m\"RDFU12\"\u001b[39m, \u001b[32m0.0\u001b[39m),\n",
       "  (\u001b[32m\"RDFU29\"\u001b[39m, \u001b[32m0.0\u001b[39m),\n",
       "  (\u001b[32m\"RDFU7\"\u001b[39m, \u001b[32m0.0\u001b[39m),\n",
       "  (\u001b[32m\"RNCS\"\u001b[39m, \u001b[32m0.0\u001b[39m),\n",
       "  (\u001b[32m\"Vm\"\u001b[39m, \u001b[32m0.0\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viPairs.takeRight(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres20\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m86\u001b[39m"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viPairs.count { case (_, vi) => vi > 0.01 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
